#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Aug 17 09:35:04 2023

@author: d

Sources: https://pyod.readthedocs.io/en/latest/
         https://github.com/SeldonIO/alibi-detect
         https://github.com/vc1492a/PyNomaly

requirements:
    conda create -n od spyder -c conda-forge
    pip install seaborn matplotlib pyod pynomaly scikit-learn alibi-detect
"""

#%% Libraries

import seaborn as sns

from IPython.display import display
from matplotlib import pyplot as plt
from PIL import Image
from pyod.models.abod import ABOD
from pyod.models.cblof import CBLOF
from PyNomaly import loop
from sklearn.preprocessing import minmax_scale

from alibi_detect.od import IForest

#%%

# =============================================================================
# Outlier Detection with pyod, alibi-detect, and PyNomaly
#
# The general idea of outlier detection is to identify data
# objects that do not fit well in the general data distributions.
#
# The reasoning is that data objects (observations) are generated by certain
# mechanisms or statistical processes. Distinct deviations from the main
# distributions then are supposed to originate from a different mechanism.
#
# Problem:  "One person’s noise is another person’s signal"
# =============================================================================

# Load Tips Dataset
df = sns.load_dataset('tips')
print(df.head())

# Plot Tips Dataset
plt.figure(dpi=300)
sns.scatterplot(data = df, x = 'total_bill', y = 'tip')
plt.title("Tips Dataset")
plt.show()

#%%

# =============================================================================
# 1. pyod package
# 
# - more than 30 algorithms, from classical to the most latest (-> up to date)
# - Algorithms of pyod package are said to be fast, but accuracy of outlier
#   detection is heavily dependent on the type of data
# =============================================================================

# Angle based outlier detection
# For a point within a cluster, the angles between different vectors to pairs
# of other points differ widely. The variance of the angles will become smaller
# for points at the border of a cluster.
# However, even here the variance is still relatively high compared to the
# variance of angles for real outliers. Here, the angles to most pairs of
# points will be small since most points are clustered in some directions.

# -> adresses the problem of distance-based measures with high dimensional data 
#    as they become less meaningful with high dimensional data 
#    (in high dimensional data all points are kind of sparse)
#    https://www.dbs.ifi.lmu.de/Publikationen/Papers/KDD2008.pdf

im = Image.open("ABOD_intuition.jpg")
display(im)

#%% Implementation

# Contamination is an important parameter, that has to be set in advance
abod_clf = ABOD(contamination=0.05)
abod_clf.fit(df[['total_bill', 'tip']])

# Return the classified inlier/outlier
abod_clf.labels_
# Return decision scores (measure for likelihood of being an outlier)
abod_clf.decision_scores_

# Store labels in df 
df['ABOD_Clf'] = abod_clf.labels_

# Plot with hue = "label"
plt.figure(dpi=300)
sns.scatterplot(data = df, x = 'total_bill', y = 'tip', hue = 'ABOD_Clf')
plt.title("ABOD")
plt.show()

#%% Cluster-based local outlier factor

# - The first step assigns a data point to one and only one cluster.
# - The second step ranks clusters according to cluster size from large to
#   small and get the cumulative data counts. Those clusters that hold up to
#   90% of the data are called the “large” clusters, and the clusters that
#   hold the remaining 10% are called the “small” clusters. The threshold
#   of 90% can be fine-tuned in the modeling.
# - The third step calculates the distance of a data point to the centroid
#   and the outlier score according to the following rule:

# -> If a data point belongs to a large cluster, the distance is the distance
#    to the centroid of its cluster.
#    The outlier score is the distance multiplied by the number of data points
#    in the cluster.
 
# -> If a data point belongs to a small cluster, the distance is the distance
#    to the centroid of the nearest large cluster. 
#    The outlier score is the distance multiplied by the amount of data of the
#    small cluster to which the data point belongs.

im = Image.open("CBLOF_intuition.jpg")
display(im)

#%% Implementation

cblof_clf = CBLOF(contamination=0.05, random_state=42)
cblof_clf.fit(df[['total_bill', 'tip']])

# Outlier labels and decision scores
df['CBLOF_Clf'] = cblof_clf.labels_
cblof_clf.decision_scores_

# Plot
plt.figure(dpi=300)
sns.scatterplot(data = df, x = 'total_bill', y = 'tip', hue = 'CBLOF_Clf')
plt.title("CBLOF")
plt.show()

#%% Chose outliers based on a measure of probability

# Use min_max_scale on decision_score vector
outlier_prob = minmax_scale(cblof_clf.decision_scores_)
df["CBLOF_prob"] = outlier_prob

df["CBLOF_out"] = df['CBLOF_prob'].apply(lambda x: 1 if x > 0.33 else 0)
plt.figure(dpi=300)
sns.scatterplot(data = df, x = 'total_bill', y = 'tip', hue = 'CBLOF_out')
plt.title("CBLOF_prob")
plt.show()

#%%

# =============================================================================
# 2. alibi detect, used for outlier, adversarial, and drift detection.
# 
# -> Can also be used for tabular and unstructured data like images or text
#
# Image outlier: https://docs.seldon.io/projects/alibi-detect/en/latest/examples/alibi_detect_deploy.html
# =============================================================================

# In an Isolation Forest, randomly sub-sampled data is processed in a tree
# structure based on randomly selected features. 
# The samples that travel deeper into the tree are less likely to be anomalies
# as they required more cuts to isolate them. 
# Similarly, the samples which end up in shorter branches indicate anomalies
# as it was easier for the tree to separate them from other observations.

    # - When given a dataset, a random sub-sample of the data is selected and
    #   assigned to a binary tree.
    
    # - Branching of the tree starts by selecting a random feature
    #   (from the set of all N features) first.
    #   And then branching is done on a random threshold (any value in the
    #   range of minimum and maximum values of the selected feature).
    
    # - If the value of a data point is less than the selected threshold,
    #   it goes to the left branch else to the right.    
    #   And thus a node is split into left and right branches.
    
    # - This process from step 2 is continued recursively till each data point
    #   is completely isolated
    #   or till max depth(if defined) is reached.
    
    # - The above steps are repeated to construct random binary trees.

im = Image.open("Isolation_forest_intuition.png")
display(im)

#%% Implementation

# Construct instance of IForest class
# Threshold parameters indicate score above a point is calssified as outlier
# n_estimators indicates the number of trees
od = IForest(threshold=0., n_estimators=100)

# Use certain dataset features to fit the forest 
od.fit(df[['total_bill', 'tip']])

# Threshold is set to classify 5 % as outliers (95 % inlier)
od.infer_threshold(df[['total_bill', 'tip']], threshold_perc=95)

# Make predictions 
preds = od.predict(
    df[['total_bill', 'tip']],
    return_instance_score=True
)

# Returns dictionary with different scores 
preds['data'].keys()

df['IF_alibi'] = preds['data']['is_outlier']
# Usage Example: df_filtered = df[df["IF_alibi"] != 1]
plt.figure(dpi=300)
sns.scatterplot(data = df, x = 'total_bill', y = 'tip', hue = 'IF_alibi')
plt.title("Isolation_Forest")
plt.show()

#%%

# =============================================================================
# Pynomaly
# A Python 3 implementation of LoOP: Local Outlier Probabilities, a local
# density based outlier detection method providing an outlier score in
# the range of [0,1].
# =============================================================================

# Loop (local outlier probability) is based on the local outlier factor (LOF)
# but provides normalized scores between 0 and 1.
# The local outlier factor is based on a concept of a local density,
# where locality is given by k nearest neighbors, whose distance is used to
# estimate the density.
# By comparing the local density of an object to the local densities of its
# neighbors, one can identify regions of similar density, and points that have
# a substantially lower density than their neighbors.

# These are considered to be outliers. 

# Advantage: local outliers can be found.

# Open the transparent image
image = Image.open("LOF_intuition.png")

# Create a new image with white background and the same size as the original image
new_image = Image.new("RGB", image.size, "white")

# Composite the transparent image onto the new image with white background
new_image.paste(image, (0, 0), image)

# Plot the new image
display(new_image)

#%%

m = loop.LocalOutlierProbability(df[['total_bill', 'tip']], progress_bar=True).fit()
scores = m.local_outlier_probabilities

df['loop_score'] = scores
df['loop_label'] = df['loop_score'].apply(lambda x: 1 if x > 0.50 else 0)

plt.figure(dpi=300)
sns.scatterplot(data = df, x = 'total_bill', y = 'tip', hue = 'loop_label')
plt.title("Loop")
plt.show()
